{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "\n",
    "from sympy import symbols, simplify, derive_by_array\n",
    "from scipy.integrate import solve_ivp\n",
    "from xLSINDy_sp import *\n",
    "from sympy.physics.mechanics import *\n",
    "from sympy import *\n",
    "from Data_generator_py import image_process\n",
    "import sympy\n",
    "import torch\n",
    "import sys\n",
    "import HLsearch as HL\n",
    "import example_pendulum_double_pendulum as example_pendulum\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value: 3216638525\n"
     ]
    }
   ],
   "source": [
    "#set the random seed for reproducibility\n",
    "seed_value = random.randint(0, 2**32 - 1)  # This generates a random integer in the range [0, 2^32 - 1]\n",
    "#seed_value = 3489499403\n",
    "#seed_value = 4027751856\n",
    "# Set the seed for numpy\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set the seed for PyTorch\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Also seed for cuda if you are using GPU computations\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Seed value: {seed_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "#set the environment for deciding the path to save the files\n",
    "environment = \"server\"\n",
    "sample_size = 10\n",
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating pendulum data, pendulum type: double pendulum\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(r'../../../HLsearch/')\n",
    "#set the parameters for the double pendulum\n",
    "params = {}\n",
    "params['adding_noise'] = False\n",
    "params['noise_type'] = 'angle_noise'\n",
    "params['noiselevel'] = 2e-2\n",
    "params['changing_length'] = False\n",
    "params['specific_random_seed'] = False\n",
    "params['c'] = float(0.18)\n",
    "params['g'] = float(9.81)\n",
    "params['L1'] = float(1.5)\n",
    "params['L2'] = float(1.5)\n",
    "params['m1'] = float(1)\n",
    "params['m2'] = float(1)\n",
    "params['b1'] = float(0.05)\n",
    "params['b2'] = float(0.05)\n",
    "if environment == 'laptop':\n",
    "    root_dir =R'C:\\Users\\87106\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'desktop':\n",
    "    root_dir = R'E:\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'server':\n",
    "    root_dir = R'/mnt/ssd1/stilrmy/Autoencoder-conservtive_expression'\n",
    "#just named as image_process, but actually it is a simple raw data pass through now\n",
    "x,dx,ddx = image_process(sample_size,params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4)\n"
     ]
    }
   ],
   "source": [
    "#store the raw data in X and Xdot variables\n",
    "X = []\n",
    "Xdot = []\n",
    "for i in range(len(x)):\n",
    "    temp_list = np.hstack([x[i,:],dx[i,:]])\n",
    "    X.append(temp_list)\n",
    "    temp_list = np.hstack([dx[i,:],ddx[i,:]])\n",
    "    Xdot.append(temp_list)\n",
    "X = np.vstack(X)\n",
    "Xdot = np.vstack(Xdot)\n",
    "print(Xdot.shape)\n",
    "#change X and Xdot dtype to float32 to match the network\n",
    "X = X.astype('float32')\n",
    "Xdot = Xdot.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states are: (x0, x1, x0_t, x1_t)\n",
      "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
     ]
    }
   ],
   "source": [
    "#setting the states and states derivatives\n",
    "states_dim = 4\n",
    "states = ()\n",
    "states_dot = ()\n",
    "for i in range(states_dim):\n",
    "    if(i<states_dim//2):\n",
    "        states = states + (symbols('x{}'.format(i)),)\n",
    "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
    "    else:\n",
    "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
    "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
    "print('states are:',states)\n",
    "print('states derivatives are: ', states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn from sympy to str\n",
    "states_sym = states\n",
    "states_dot_sym = states_dot\n",
    "states = list(str(descr) for descr in states)\n",
    "states_dot = list(str(descr) for descr in states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, targ):\n",
    "    loss = torch.mean((pred - targ)**2) \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(w, alpha):\n",
    "    clipped = torch.minimum(w,alpha)\n",
    "    clipped = torch.maximum(clipped,-alpha)\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxL1norm(w_hat, alpha):\n",
    "    if(torch.is_tensor(alpha)==False):\n",
    "        alpha = torch.tensor(alpha)\n",
    "    w = w_hat - clip(w_hat,alpha)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxSCAD(v, lam, a, lr):\n",
    "    \"\"\"Approximate Proximal operator for the SCAD penalty using Newton's method\"\"\"\n",
    "    x = v.clone()\n",
    "    for _ in range(10):  # 10 iterations for Newton's method\n",
    "        abs_x = torch.abs(x)\n",
    "        condition1 = (abs_x <= lam)\n",
    "        condition2 = (abs_x > lam) & (abs_x <= a * lam)\n",
    "        condition3 = (abs_x > a * lam)\n",
    "        \n",
    "        # Derivative of the SCAD penalty\n",
    "        derivative = torch.where(condition1, torch.sign(x),\n",
    "                                 torch.where(condition2, x / abs_x, a * x / abs_x))\n",
    "        \n",
    "        # Second derivative of the SCAD penalty\n",
    "        second_derivative = torch.where(condition1, torch.zeros_like(x),\n",
    "                                        torch.where(condition2, torch.zeros_like(x), torch.zeros_like(x)))\n",
    "        \n",
    "        # Newton's update\n",
    "        x = x - lr * (x - v + lam * derivative) / (1 + lr * second_derivative)\n",
    "        \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two sparse regression algorithms, the Prox_loop is the one that Adam used in the previous research, and the SR_loop is a Adaptive Moment Estimation optimizer(also named Adam). The SR_loop is better but neither of them are the best solution for sparse regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prox_loop(coef,d_coef,prevcoef,Zeta,Eta,Delta,Dissip,xdot,bs,lr,lam,device):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    v = coef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev = prevcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    for i in range(tl//bs):\n",
    "        vhat = (v + ((i - 1) / (i + 2)) * (v - prev)).clone().detach().requires_grad_(True)\n",
    "        prev = v\n",
    "        zeta = Zeta[:,:,:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,:,:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,:,i*bs:(i+1)*bs]\n",
    "        dissip = Dissip[:,:,i*bs:(i+1)*bs]\n",
    "        x_t = torch.tensor(xdot[i*bs:(i+1)*bs,:]).to(device)\n",
    "        q_tt = x_t[:,n//2:]\n",
    "        d_coef = torch.tensor([0.025,0.025]).to(device).float()\n",
    "        loss = lagrangianforward(vhat,d_coef,zeta,eta,delta,dissip,x_t,device)\n",
    "        loss = torch.mean(loss**2)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            v = vhat - lr * vhat.grad\n",
    "            v = proxSCAD(v,lam,3.7,lr)\n",
    "            vhat.grad = None\n",
    "        loss_list.append(loss)\n",
    "    return v,prev,torch.tensor(loss_list).mean().item(),tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SR_loop(coef,d_coef, prevcoef, predcoef, Zeta, Eta, Delta,Dissip, xdot, bs, lr, lam,d_training,beta1=0.9,beta2=0.999,eps=1e-8):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    #if(torch.is_tensor(xdot)==False):\n",
    "        #xdot = torch.from_numpy(xdot).to(device).float()\n",
    "    v = coef.clone().detach().to(device).requires_grad_(True)\n",
    "    v_d = d_coef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev = prevcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev_d = predcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    # Initialize moving averages for Adam\n",
    "    m_v = torch.zeros_like(v)\n",
    "    m_d = torch.zeros_like(v_d)\n",
    "    v_v = torch.zeros_like(v)\n",
    "    v_d_ = torch.zeros_like(v_d)\n",
    "    for i in range(tl//bs):\n",
    "        #computing acceleration with momentum\n",
    "        vhat = v.requires_grad_(True).clone().detach().to(device).requires_grad_(True)\n",
    "        vdhat = v_d.requires_grad_(True).clone().detach().to(device).requires_grad_(True)\n",
    "        prev = v\n",
    "        prev_d = v_d\n",
    "        #Computing loss\n",
    "        zeta = Zeta[:,:,:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,:,:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,:,i*bs:(i+1)*bs]\n",
    "        dissip = Dissip[:,:,i*bs:(i+1)*bs]\n",
    "        x_t = torch.tensor(xdot[i*bs:(i+1)*bs,:]).to(device)\n",
    "        vdhat_ = torch.tensor([0.025,0.025]).to(device).float()\n",
    "        #forward\n",
    "        lossval = lagrangianforward(vhat,vdhat_,zeta,eta,delta,dissip,x_t,device)\n",
    "        #lossval = torch.mean(lossval[0,:]**2)+torch.mean(lossval[1,:]**2)\n",
    "        lossval = torch.mean(lossval**2)\n",
    "        l1_norm = torch.norm(vhat, 1)\n",
    "        l1_d_norm = torch.norm(vdhat, 1)\n",
    "        lossval = lossval + lam * l1_norm \n",
    "        #Backpropagation\n",
    "        lossval.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(vhat, max_norm=6)\n",
    "        #torch.nn.utils.clip_grad_norm_(vdhat, max_norm=6)\n",
    "        with torch.no_grad():\n",
    "            # Update moving averages\n",
    "            m_v = beta1 * m_v + (1 - beta1) * vhat.grad\n",
    "            v_v = beta2 * v_v + (1 - beta2) * (vhat.grad ** 2)\n",
    "            # m_d = beta1 * m_d + (1 - beta1) * vdhat.grad\n",
    "            #v_d_ = beta2 * v_d_ + (1 - beta2) * (vdhat.grad ** 2)\n",
    "            # Compute bias-corrected moving averages\n",
    "            m_v_hat = m_v / (1 - beta1 ** (i + 1))\n",
    "            v_v_hat = v_v / (1 - beta2 ** (i + 1))\n",
    "            m_d_hat = m_d / (1 - beta1 ** (i + 1))\n",
    "            v_d_hat = v_d_ / (1 - beta2 ** (i + 1))\n",
    "            # Update parameters\n",
    "            v = vhat - lr * m_v_hat / (torch.sqrt(v_v_hat) + eps)\n",
    "            #v_d = vdhat - lr * m_d_hat / (torch.sqrt(v_d_hat) + eps)\n",
    "            #reset gradient\n",
    "            vhat.grad.zero_()\n",
    "            #vdhat.grad.zero_()\n",
    "\n",
    "        loss_list.append(lossval.item())\n",
    "    return v, vdhat_, prev, prev_d, torch.tensor(loss_list).mean().item(),tl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a sanity check to see if any of the right candiates are missing\n",
    "def check_candidates(expr_temp):\n",
    "    candidates = ['x0_t**2', 'x1_t**2', 'cos(x0)', 'cos(x1)', 'x0_t*x1_t*cos(x0)*cos(x1)', 'x0_t*x1_t*sin(x0)*sin(x1)']\n",
    "    for candidate in candidates:\n",
    "        if candidate not in expr_temp:\n",
    "            return False\n",
    "            print('candidate {} not in expr_temp'.format(candidate))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables to keep track of success and total trials\n",
    "total_trials = 100  # or any number you'd like\n",
    "successful_trials = 0\n",
    "\n",
    "# Loop for each trial\n",
    "for trial in range(total_trials):\n",
    "    #set the random seed for reproducibility\n",
    "    seed_value = random.randint(0, 2**32 - 1)  # This generates a random integer in the range [0, 2^32 - 1]\n",
    "    #seed_value = 3489499403\n",
    "    #seed_value = 4027751856\n",
    "    # Set the seed for numpy\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # Set the seed for PyTorch\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    # Also seed for cuda if you are using GPU computations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(f\"Seed value: {seed_value}\")\n",
    "    # ... [Your existing code for setting up the problem, like building function expressions, goes here]\n",
    "    # build function expression for the library in str\n",
    "    exprdummy = HL.buildFunctionExpressions(1,states_dim,states,use_sine=True)\n",
    "    polynom = exprdummy[2:4]\n",
    "    trig = exprdummy[4:]\n",
    "    polynom = HL.buildFunctionExpressions(2,len(polynom),polynom)\n",
    "    trig = HL.buildFunctionExpressions(2, len(trig),trig)\n",
    "    product = []\n",
    "    for p in polynom:\n",
    "        for t in trig:\n",
    "            product.append(p + '*' + t)\n",
    "    expr = polynom + trig + product\n",
    "    expr = np.array(expr)\n",
    "    #check Adam's xLSINDy paper for why we need to delete some of the expressions\n",
    "    i2 = np.where(expr == 'x0_t**2*cos(x0)**2')[0]\n",
    "    i3 = np.where(expr == 'x0_t**2*cos(x1)**2')[0]\n",
    "    i7 = np.where(expr == 'x1_t*cos(x0)**2')[0]\n",
    "    i8 = np.where(expr == 'x1_t*cos(x1)**2')[0]\n",
    "    i9 = np.where(expr == 'x1_t')[0]\n",
    "    i10 = np.where(expr == 'x0_t*cos(x0)**2')[0]\n",
    "    i11 = np.where(expr == 'x0_t*cos(x1)**2')[0]\n",
    "    i12 = np.where(expr == 'x0_t')[0]\n",
    "    i13 = np.where(expr == 'cos(x0)**2')[0]\n",
    "    i14 = np.where(expr == 'cos(x1)**2')[0]\n",
    "    i15 = np.where(expr == 'sin(x0)**2')[0]\n",
    "    i16 = np.where(expr == 'sin(x1)**2')[0]\n",
    "    i17 = np.where(expr == 'x0_t*x1_t')[0]\n",
    "    idx = np.arange(0,len(expr))\n",
    "    idx = np.delete(idx,[i2,i3,i7,i8,i9,i10,i11,i12,i13,i14,i15,i16,i17])\n",
    "\n",
    "    expr = np.delete(expr,[i2,i3,i7,i8,i9,i10,i11,i12,i13,i14,i15,i16,i17])\n",
    "    #expr = ['x0_t**2','x1_t**2','cos(x0)','cos(x1)','x0_t*x1_t*cos(x0)*cos(x1)','x0_t*x1_t*sin(x0)*sin(x1)']\n",
    "    #library function expression for the dissipation function in str\n",
    "    #d_expr = [ 'x0_t','x0_t**2','x0_t**3','x1_t','x1_t**2','x1_t**3','x0_t*x1_t']\n",
    "    d_expr = ['x0_t**2','x1_t**2']\n",
    "    print(expr)\n",
    "    print(d_expr)\n",
    "    # Loop for the training process (or whatever your existing loop structure is)\n",
    "    #building the partial deriative of the library function expression and calculate with the raw data\n",
    "    Zeta, Eta, Delta, Dissip = LagrangianLibraryTensor(X,Xdot,expr,d_expr,states,states_dot,device,scaling=True)\n",
    "    Eta = Eta.to(device)\n",
    "    Zeta = Zeta.to(device)\n",
    "    Delta = Delta.to(device)\n",
    "    Dissip = Dissip.to(device)\n",
    "    #coefficients for the Lagrangian\n",
    "    mask = torch.ones(len(expr),device=device)\n",
    "    xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
    "    #xi_L = torch.ones(len(expr), device=device)\n",
    "    xi_L = xi_L.type(torch.FloatTensor)\n",
    "    prevxi_L = xi_L.clone().detach()\n",
    "    # coefficients for the dissipation function\n",
    "    d_mask = torch.ones(len(d_expr),device=device)\n",
    "    xi_d = torch.ones(len(d_expr),device=device)*0\n",
    "    prevxi_d = xi_d.clone().detach()\n",
    "    threshold = 0.001\n",
    "    threshold_d = 0.001\n",
    "    num_candidates_removed = 0\n",
    "    while True:\n",
    "        # ... [Your existing training or optimization code goes here]\n",
    "        #Redefine computation after thresholding\n",
    "        Zeta, Eta, Delta, Dissip = LagrangianLibraryTensor(X,Xdot,expr,d_expr,states,states_dot,device,scaling=True)\n",
    "        Eta = Eta.to(device)\n",
    "        Zeta = Zeta.to(device)\n",
    "        Delta = Delta.to(device)\n",
    "        # Dissip = Dissip.to(device)\n",
    "        # print(expr)\n",
    "        # print(d_expr)\n",
    "        # print(\"Zeta:\",Zeta)\n",
    "        # print(\"Eta:\",Eta)\n",
    "        # print(\"Delta:\",Delta)\n",
    "        # print(\"Dissip:\",Dissip)\n",
    "\n",
    "        #Training\n",
    "        Epoch = 200\n",
    "        i = 1\n",
    "        lr = 1e-5\n",
    "        if len(xi_L) <= 25:\n",
    "            reset_threshold = 400\n",
    "            threshold = 0.01\n",
    "        else:\n",
    "            reset_threshold = 120\n",
    "        if(stage==1):\n",
    "            lam = 0\n",
    "        else:\n",
    "            lam = 1e-3\n",
    "        if len(xi_L) <= 25:\n",
    "            lam = 5e-3\n",
    "        #set the lam to zero when mask is equal to 11\n",
    "        d_training = False\n",
    "        temp = 1000\n",
    "        if len(xi_L) == 6:\n",
    "            threshold = 0\n",
    "            lam = 0\n",
    "        while(i<=Epoch):   \n",
    "            #xi_L , xi_d, prevxi_L, prevxi_d, lossitem, q= SR_loop(xi_L,xi_d,prevxi_L,prevxi_d,Zeta,Eta,Delta,Dissip,Xdot,2500,lr,lam,d_training)\n",
    "            xi_L,prevxi_L,lossitem,q = Prox_loop(xi_L,xi_d,prevxi_L,Zeta,Eta,Delta,Dissip,Xdot,500,lr,lam,device)\n",
    "            if i %200 == 0:\n",
    "                print(\"\\n\")\n",
    "                print(\"Stage \",stage)\n",
    "                print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "                print(\"Learning rate : \", lr)\n",
    "                print(\"Average loss : \" , lossitem)\n",
    "            temp = lossitem\n",
    "            if temp <= 5:\n",
    "                lr = 1e-5\n",
    "            if temp <= 2:\n",
    "                lr = 1e-5\n",
    "            if temp <= 0.05:\n",
    "                lr = 1e-5\n",
    "\n",
    "            i+=1    \n",
    "        surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "        expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "        xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "        num_candidates_removed += len(prevxi_L) - len(xi_L)\n",
    "        prevxi_L = xi_L.clone().detach()\n",
    "        mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "        if num_candidates_removed >= reset_threshold:\n",
    "            xi_L = torch.ones(len(expr), device=device).data.uniform_(5,7)\n",
    "            prevxi_L = xi_L.clone().detach()\n",
    "            num_candidates_removed = 0\n",
    "\n",
    "        xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=4)\n",
    "        xi_dcpu = np.around(xi_d.detach().cpu().numpy(),decimals=4)\n",
    "        L = HL.generateExpression(xi_Lcpu,expr)\n",
    "        D = HL.generateExpression(xi_dcpu,d_expr)\n",
    "        print(\"expression length:\\t\",len(xi_L))\n",
    "        print(\"Result stage \" + str(stage+2) + \":\" , L)\n",
    "        print(\"Dissipation function:\", D)\n",
    "        print(\"removed candidates:\", num_candidates_removed)\n",
    "        print(\"sanity check: \",check_candidates(expr))\n",
    "        # Check the conditions\n",
    "        num_candidates = len(xi_L)  # Retrieve the current number of candidates\n",
    "\n",
    "        # Call the check_candidates function and store its output\n",
    "        check_result = check_candidates(expr)  # Your existing check_candidates function call\n",
    "\n",
    "        # Check for success criteria\n",
    "        if num_candidates == 6 and check_result:\n",
    "            print(f\"Trial {{trial + 1}} is successful.\")\n",
    "            successful_trials += 1\n",
    "            break  # Exit the training loop for this trial\n",
    "\n",
    "        # Check for termination criteria based on check_candidates output\n",
    "        if not check_result:\n",
    "            print(f\"Trial {{trial + 1}} failed.\")\n",
    "            break  # Exit the training loop for this trial\n",
    "\n",
    "# Compute and display the success rate\n",
    "success_rate = (successful_trials / total_trials) * 100\n",
    "print(f\"The success rate is {{success_rate}}%.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
