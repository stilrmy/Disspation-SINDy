{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "\n",
    "from sympy import symbols, simplify, derive_by_array, cos, sin, sympify\n",
    "from scipy.integrate import solve_ivp\n",
    "from xLSINDy import *\n",
    "from sympy.physics.mechanics import *\n",
    "from sympy import *\n",
    "from adaPGM import adaptive_primal_dual, NormL1, Zero, OurRule\n",
    "import sympy\n",
    "import torch\n",
    "import HLsearch as HL\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(func, time, init_values):\n",
    "    sol = solve_ivp(func,[time[0],time[-1]],init_values,t_eval=time, method='RK45',rtol=1e-10,atol=1e-10)\n",
    "    return sol.y.T, np.array([func(0,sol.y.T[i,:]) for i in range(sol.y.T.shape[0])],dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pendulum(t,x):\n",
    "    return x[1],-9.81*np.sin(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartPendulum2_wrapper(params):\n",
    "    def cartPendulum2(t, y):\n",
    "        l, M, m, b1, b2, tau, omega, phi,g = params['L'], params['M'], params['m'], params['b1'], params['b2'], params['tau'],params['omega'] , params['phi'], params['g']\n",
    "\n",
    "        theta,x,thetadot,xdot = y\n",
    "        F = tau*np.cos(omega*t)\n",
    "        num = (F + m * g * np.sin(theta) * np.cos(theta) + (b2 / l) * thetadot * np.cos(theta) + m * l * thetadot**2 * np.sin(theta) - b1 * xdot)\n",
    "        denom = (M + m - m * l * np.cos(theta)**2)\n",
    "        xdotdot = num / denom\n",
    "    \n",
    "        # Compute theta_ddot\n",
    "        thetadotdot = (-g / l * np.sin(theta) - xdotdot * np.cos(theta) - (b2 / (m * l**2)) * thetadot)\n",
    "        # xdotdot = (tau*np.cos(omega*t)+m*np.sin(theta)*(l*thetadot**2+g*np.cos(theta))-b1*xdot)/(M+m*(np.sin(theta)**2))\n",
    "\n",
    "        # thetadotdot = (-tau*np.cos(omega*t)*np.cos(theta) - m*l*thetadot**2*np.sin(theta)*np.cos(theta) - (M+m)*g*np.sin(theta)-b2*thetadot)/(l*(M+m*(np.sin(theta)**2)))\n",
    "\n",
    "\n",
    "        return thetadot,xdot,thetadotdot,xdotdot\n",
    "    return cartPendulum2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartpole(t,y,f=0.0):\n",
    "    mc,mp,g = 1, 0.5, 9.81\n",
    "    l = 1\n",
    "    theta,x,thetadot,xdot = y\n",
    "    f = 0.1 * np.cos(t)\n",
    "    xdotdot = (f+mp*np.sin(theta)*(l*thetadot**2+g*np.cos(theta)))/(mc+mp*np.sin(theta)**2)\n",
    "    thetadotdot = (-f*np.cos(theta)-mp*l*thetadot**2*np.cos(theta)*np.sin(theta)-(mc+mp)*g*np.sin(theta))/(l*(mc+mp*np.sin(theta)**2))\n",
    "    return thetadot,xdot,thetadotdot,xdotdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, targ):\n",
    "    loss = torch.mean((pred - targ)**2) \n",
    "    return loss \n",
    "\n",
    "\n",
    "def clip(w, alpha):\n",
    "    clipped = torch.minimum(w,alpha)\n",
    "    clipped = torch.maximum(clipped,-alpha)\n",
    "    return clipped\n",
    "\n",
    "def proxL1norm(w_hat, alpha, nonpenaltyidx):\n",
    "    if(torch.is_tensor(alpha)==False):\n",
    "        alpha = torch.tensor(alpha)\n",
    "    w = w_hat - clip(w_hat,alpha)\n",
    "    for idx in nonpenaltyidx:\n",
    "        w[idx] = w_hat[idx]\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanity_check(expr):\n",
    "    real_candidates = ['x1_t**2', 'cos(x0)']\n",
    "    found = {cand: False for cand in real_candidates}  # Dictionary to track found items\n",
    "    \n",
    "    for cand in real_candidates:\n",
    "        for item in expr:\n",
    "            if cand in item:\n",
    "                found[cand] = True  # Mark as found\n",
    "    \n",
    "    # Check if any candidate was not found\n",
    "    for cand, is_found in found.items():\n",
    "        if not is_found:\n",
    "            print(\"Lacking of term:\", cand)\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(param=None,device='cuda:4',opt_mode='PGD',num_sample=100,noiselevel=0,Epoch=100,Epoch0=100,lr=1e-5,lr_step=1e-6,lam0=1,lam=0.2,batch_size=128,threshold_d=0,tol=1e-5,display=True):\n",
    "#default setting, works well for most cases\n",
    "# def main(param=None,device='cuda:0',opt_mode='PGD',num_sample=100,noiselevel=0,Epoch=100,Epoch0=100,lr=4e-6,lr_step=1e-6,lam0=0.8,lam=0.1,batch_size=128,threshold_d=0):\n",
    "#optuna best setting\n",
    "# def main(param=None,device='cuda:0',opt_mode='PGD',num_sample=73,noiselevel=0,Epoch=231,Epoch0=348,lr=1.1e-6,lr_step=6e-6,lam0=0.8,lam=0.248,batch_size=256):\n",
    "# device = 'cuda:7'\n",
    "param=None\n",
    "device='cuda:5'\n",
    "opt_mode='PGD'\n",
    "num_sample=100\n",
    "noiselevel=1e-1\n",
    "Epoch=100\n",
    "Epoch0=100\n",
    "lr=1e-5\n",
    "lr_step=1e-6\n",
    "lam0=1\n",
    "lam=0.2\n",
    "batch_size=128\n",
    "threshold_d=0\n",
    "tol=1e-5\n",
    "display=True\n",
    "if param is None:\n",
    "    param = {}\n",
    "    param['L'] = 1\n",
    "    param['M'] = 1\n",
    "    param['m'] = 0.5\n",
    "    param['b1'] = 0.5\n",
    "    param['b2'] = 0.5\n",
    "    param['tau'] = 0.1\n",
    "    param['omega'] = 1\n",
    "    param['phi'] = 0\n",
    "    param['g'] = 9.81\n",
    "# The gravitational acceleration (m.s-2).\n",
    "\n",
    "cartPendulum = cartPendulum2_wrapper(param)\n",
    "#Saving Directory\n",
    "rootdir = \"../Double Pendulum/Data/\"\n",
    "create_data = True\n",
    "training = True\n",
    "save = False\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Data\n"
     ]
    }
   ],
   "source": [
    "if(create_data):\n",
    "        if display:\n",
    "            print(\"Creating Data\")\n",
    "        X, Xdot = [], []\n",
    "        for i in range(num_sample):\n",
    "            t = np.arange(0,5,0.01)\n",
    "            theta = np.random.uniform(-np.pi, np.pi)\n",
    "            thetadot = np.random.uniform(0,0)\n",
    "\n",
    "            \n",
    "            y0=np.array([theta,thetadot,0,0])\n",
    "            x,xdot = generate_data(cartPendulum,t,y0)\n",
    "            X.append(x)\n",
    "            Xdot.append(xdot)\n",
    "        X = np.vstack(X)\n",
    "        Xdot = np.vstack(Xdot)\n",
    "        #genrate sinodusal input using the omega and tau0\n",
    "        Tau = np.array([np.zeros_like(t),param['tau']*np.cos(param['omega']*t)])\n",
    "        Tau = torch.tensor(Tau,device=device).float()\n",
    "        #duplicate the input to match the size of the data\n",
    "        Tau_temp = Tau\n",
    "        for i in range(num_sample-1):\n",
    "            Tau_temp = torch.cat((Tau_temp, Tau), dim=1)\n",
    "        Tau = Tau_temp\n",
    "        Tau_org = Tau.clone()\n",
    "        if(save==True):\n",
    "            np.save(rootdir + \"X.npy\", X)\n",
    "            np.save(rootdir + \"Xdot.npy\",Xdot)\n",
    "else:\n",
    "    X = np.load(rootdir + \"X.npy\")\n",
    "    Xdot = np.load(rootdir + \"Xdot.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states are: (x0, x1, x0_t, x1_t)\n",
      "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
     ]
    }
   ],
   "source": [
    "#adding noise\n",
    "mu, sigma = 0, noiselevel\n",
    "noise = np.random.normal(mu, sigma, X.shape[0])\n",
    "for i in range(X.shape[1]):\n",
    "    X[:,i] = X[:,i]+noise\n",
    "    Xdot[:,i] = Xdot[:,i]+noise\n",
    "\n",
    "\n",
    "states_dim = 4\n",
    "states = ()\n",
    "states_dot = ()\n",
    "for i in range(states_dim):\n",
    "    if(i<states_dim//2):\n",
    "        states = states + (symbols('x{}'.format(i)),)\n",
    "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
    "    else:\n",
    "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
    "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
    "if display:\n",
    "    print('states are:',states)\n",
    "    print('states derivatives are: ', states_dot)\n",
    "#Turn from sympy to str\n",
    "states_sym = states\n",
    "states_dot_sym = states_dot\n",
    "states = list(str(descr) for descr in states)\n",
    "states_dot = list(str(descr) for descr in states_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression :  ['x0_t', 'sin(x0)', 'cos(x0)', 'x1', 'x1_t', 'x0_t**2', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2', 'x0_t*x1', 'sin(x0)*x1', 'cos(x0)*x1', 'x1**2', 'x0_t*x1_t', 'sin(x0)*x1_t', 'cos(x0)*x1_t', 'x1*x1_t', 'x1_t**2', 'x0_t**3', 'x0_t**2*sin(x0)', 'x0_t*sin(x0)**2', 'sin(x0)**3', 'x0_t**2*cos(x0)', 'x0_t*sin(x0)*cos(x0)', 'sin(x0)**2*cos(x0)', 'x0_t*cos(x0)**2', 'sin(x0)*cos(x0)**2', 'cos(x0)**3', 'x0_t**2*x1', 'x0_t*sin(x0)*x1', 'sin(x0)**2*x1', 'x0_t*cos(x0)*x1', 'sin(x0)*cos(x0)*x1', 'cos(x0)**2*x1', 'x0_t*x1**2', 'sin(x0)*x1**2', 'cos(x0)*x1**2', 'x1**3', 'x0_t**2*x1_t', 'x0_t*sin(x0)*x1_t', 'sin(x0)**2*x1_t', 'x0_t*cos(x0)*x1_t', 'sin(x0)*cos(x0)*x1_t', 'cos(x0)**2*x1_t', 'x0_t*x1*x1_t', 'sin(x0)*x1*x1_t', 'cos(x0)*x1*x1_t', 'x1**2*x1_t', 'x0_t*x1_t**2', 'sin(x0)*x1_t**2', 'cos(x0)*x1_t**2', 'x1*x1_t**2', 'x1_t**3']\n"
     ]
    }
   ],
   "source": [
    "#Separating states of pendulum and cart\n",
    "pendulum_states = []\n",
    "cartpole_states = []\n",
    "for i in range(states_dim):\n",
    "    if(i%2==0):\n",
    "        pendulum_states.append(states[i])\n",
    "    else:\n",
    "        cartpole_states.append(states[i])\n",
    "\n",
    "#build function expression for the library in str\n",
    "pend_terms = HL.buildFunctionExpressions(1,states_dim//2,pendulum_states,use_sine=True)\n",
    "cartpole_terms = HL.buildFunctionExpressions(1,states_dim//2,cartpole_states,use_sine=False)\n",
    "\n",
    "#Assuming we get a prior knowledge about a single pendulum equations\n",
    "temp = pend_terms[1:] + cartpole_terms\n",
    "expr = HL.buildFunctionExpressions(3,len(temp),temp)\n",
    "d_expr = ['x0_t**2','x1_t**2']\n",
    "print(len(expr))\n",
    "print(expr)\n",
    "exit()\n",
    "if display:\n",
    "    print(\"Expression : \", expr)\n",
    "# expr = ['cos(x0)','cos(x1)','x0_t*x1_t*cos(x0)*cos(x1)','x0_t*x1_t*sin(x0)*sin(x1)','x0_t**2','x1_t**2']\n",
    "\n",
    "\n",
    "#Creating library tensor\n",
    "Zeta, Eta, Delta, Dissip = LagrangianLibraryTensor(X,Xdot,expr,d_expr,states,states_dot, scaling=True)\n",
    "\n",
    "\n",
    "expr = np.array(expr)\n",
    "i0 = np.where(expr == 'x0_t**2')[0]\n",
    "i1 = np.where(expr == 'cos(x0)')[0]\n",
    "i2 = np.where(expr == 'x0_t**2*cos(x0)')[0]\n",
    "idx = np.arange(0,len(expr))\n",
    "delete_idx = [i0,i2]\n",
    "idx = np.delete(idx,delete_idx)\n",
    "known_expr = expr[i0].tolist()  \n",
    "expr = np.delete(expr,delete_idx).tolist()\n",
    "#non-penalty index from prev knowledge\n",
    "\n",
    "\n",
    "\n",
    "nonpenaltyidx = [i1]\n",
    "# nonpenaltyidx = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Moving to Cuda\n",
    "\n",
    "Zeta_ = Zeta[:,:,i0,:].clone().detach()\n",
    "Eta_ = Eta[:,:,i0,:].clone().detach()\n",
    "Delta_ = Delta[:,i0,:].clone().detach()\n",
    "\n",
    "Zeta = Zeta[:,:,idx,:]\n",
    "Eta = Eta[:,:,idx,:]\n",
    "Delta = Delta[:,idx,:]\n",
    "Dissip = Dissip.to(device)\n",
    "Zeta = Zeta.to(device)\n",
    "Eta = Eta.to(device)\n",
    "Delta = Delta.to(device)\n",
    "\n",
    "Zeta_ = Zeta_.to(device)\n",
    "Eta_ = Eta_.to(device)\n",
    "Delta_ = Delta_.to(device)\n",
    "\n",
    "\n",
    "\n",
    "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "xi_d = torch.ones(len(d_expr), device=device)\n",
    "c = torch.ones(len(known_expr), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_loop(Tau, c, coef, prevcoef,d_coef, RHS, LHS, Dissip, xdot, bs, lr, lam, momentum=True, D_CAL=False,device='cuda:0'):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    Zeta_, Eta_, Delta_ = LHS\n",
    "    Zeta, Eta, Delta = RHS\n",
    "    if(torch.is_tensor(xdot)==False):\n",
    "        xdot = torch.from_numpy(xdot).to(device).float()\n",
    "    \n",
    "    v = coef.clone().detach().requires_grad_(True)\n",
    "    d = d_coef.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    prev = v\n",
    "    pre_d = d\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(tl//bs):\n",
    "                \n",
    "        #computing acceleration with momentum\n",
    "        if(momentum==True):\n",
    "            vhat = (v + ((i-1)/(i+2))*(v - prev)).clone().detach().requires_grad_(True)\n",
    "            dhat = (d + ((i-1)/(i+2))*(d - pre_d)).clone().detach().requires_grad_(True)\n",
    "            dhat = d.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "        else:\n",
    "            vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "            dhat = d.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "\n",
    "        prev = v\n",
    "        pre_d = d\n",
    "\n",
    "        #Computing loss\n",
    "        zeta = Zeta[:,:,:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,:,:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,:,i*bs:(i+1)*bs]\n",
    "\n",
    "        zeta_ = Zeta_[:,:,:,i*bs:(i+1)*bs]\n",
    "        eta_ = Eta_[:,:,:,i*bs:(i+1)*bs]\n",
    "        delta_ = Delta_[:,:,i*bs:(i+1)*bs]\n",
    "\n",
    "        dissip = Dissip[:,:,i*bs:(i+1)*bs]\n",
    "        tau = Tau[:,i*bs:(i+1)*bs]\n",
    "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
    "\n",
    "\n",
    "\n",
    "        #forward\n",
    "        pred = -ELforward(vhat,zeta,eta,delta,x_t,device)\n",
    "        if D_CAL:\n",
    "            disp = DPforward(dhat,dissip,device)\n",
    "            targ = ELforward(c,zeta_,eta_,delta_,x_t,device)+disp-tau\n",
    "        else:\n",
    "            targ = ELforward(c,zeta_,eta_,delta_,x_t,device) + tau\n",
    "        lossval = loss(pred, targ)\n",
    "        \n",
    "        #Backpropagation\n",
    "        lossval.backward()\n",
    "        with torch.no_grad():\n",
    "            v = vhat - lr * vhat.grad\n",
    "            v = (proxL1norm(v, lr*lam, nonpenaltyidx))\n",
    "            if D_CAL:\n",
    "                d = dhat - lr * dhat.grad\n",
    "                d = (proxL1norm(d, lr*lam, nonpenaltyidx))\n",
    "            #reset gradient\n",
    "            vhat.grad = None\n",
    "            dhat.grad = None\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        loss_list.append(lossval.item())\n",
    "    if display:\n",
    "        print(\"Average loss : \" , torch.tensor(loss_list).mean().item())\n",
    "    return v, prevcoef,d, torch.tensor(loss_list).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 0/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  177.75782775878906\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 1/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  89.01569366455078\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 2/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  60.68547821044922\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 3/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  49.530147552490234\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 4/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  43.8350715637207\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 5/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  40.24428939819336\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 6/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  37.36310958862305\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 7/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  34.7434196472168\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 8/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  32.37251281738281\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 9/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  30.181058883666992\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 10/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  28.169450759887695\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 11/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  26.332813262939453\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 12/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  24.64808464050293\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 13/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  23.1018009185791\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 14/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  21.73329734802246\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 15/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  20.607582092285156\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 16/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  19.5827693939209\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 17/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  18.646514892578125\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 18/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  17.79044532775879\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 19/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  17.0120906829834\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 20/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  16.310014724731445\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 21/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  15.67546558380127\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 22/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  15.095113754272461\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 23/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  14.573390007019043\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 24/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  14.114130020141602\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 25/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.69987964630127\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 26/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.331507682800293\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 27/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.019987106323242\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 28/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.790130615234375\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 29/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.583952903747559\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 30/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.425844192504883\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 31/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.301434516906738\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 32/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.254961967468262\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 33/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.308860778808594\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 34/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.420517921447754\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 35/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.50777530670166\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 36/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.550172805786133\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 37/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.59156608581543\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 38/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.636439323425293\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 39/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.682031631469727\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 40/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.734447479248047\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 41/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.792848587036133\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 42/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.85595703125\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 43/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.908915519714355\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 44/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.944238662719727\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 45/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.978586196899414\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 46/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.012992858886719\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 47/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.048392295837402\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 48/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.056127548217773\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 49/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.042749404907227\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 50/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.025050163269043\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 51/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.006755828857422\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 52/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.999799728393555\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 53/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.981392860412598\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 54/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.964071273803711\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 55/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.946196556091309\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 56/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.93558406829834\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 57/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.910955429077148\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 58/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.860672950744629\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 59/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.808097839355469\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 60/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.76286792755127\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 61/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.707374572753906\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 62/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.657573699951172\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 63/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.608000755310059\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 64/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.55712890625\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 65/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.513331413269043\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 66/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.457271575927734\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 67/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.413570404052734\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 68/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.36936092376709\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 69/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.320319175720215\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 70/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.277641296386719\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 71/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.232497215270996\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 72/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.179366111755371\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 73/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.140130043029785\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 74/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.099064826965332\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 75/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.04132080078125\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 76/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  12.004737854003906\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 77/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.96245288848877\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 78/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.917972564697266\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 79/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.880096435546875\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 80/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.83670711517334\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 81/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.79974365234375\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 82/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.76136589050293\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 83/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.7186861038208\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 84/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.676013946533203\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 85/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.641901016235352\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 86/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.605680465698242\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 87/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.56747055053711\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 88/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.523252487182617\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 89/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.485316276550293\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 90/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.448505401611328\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 91/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.406789779663086\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 92/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.377398490905762\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 93/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.340455055236816\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 94/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.307780265808105\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 95/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.270590782165527\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 96/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.24129581451416\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 97/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.207326889038086\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 98/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.174445152282715\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 99/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.13979721069336\n",
      "sanity check True\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 100/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  11.106941223144531\n",
      "sanity check True\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "temp = 1000\n",
    "RHS = [Zeta, Eta, Delta]\n",
    "LHS = [Zeta_, Eta_, Delta_]\n",
    "while(i<=Epoch0):\n",
    "    if display:\n",
    "        print(\"\\n\")\n",
    "        print(\"Stage 1\")\n",
    "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch0))\n",
    "        print(\"Learning rate : \", lr)\n",
    "    xi_L, prevxi_L,xi_d, lossitem= PGD_loop(Tau, c, xi_L,prevxi_L,xi_d, RHS, LHS, Dissip, Xdot, batch_size, lr=lr,lam=lam0,momentum=True,device=device)\n",
    "    temp = lossitem\n",
    "    i+=1\n",
    "    if display:\n",
    "        print(\"sanity check\", sanity_check(expr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result stage 1:  1.44*x0_t*x1_t*cos(x0) + 0.4*x0_t*x1_t + 1.08*x1_t**2 + 18.64*cos(x0)\n"
     ]
    }
   ],
   "source": [
    "## Thresholding\n",
    "threshold = 0.01\n",
    "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "xi_d = xi_d.clone().detach().requires_grad_(True)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "\n",
    "## obtaining analytical model\n",
    "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-3)\n",
    "if display:\n",
    "    print(\"Result stage 1: \", simplify(L))\n",
    "\n",
    "last_ten_loss = []\n",
    "converged = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 0/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  5.835347652435303\n",
      "xi_L tensor([18.6720,  0.1146,  1.4861,  0.9977], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 1/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.797776460647583\n",
      "xi_L tensor([18.7053,  0.1045,  1.5732,  1.0251], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 2/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.7328271865844727\n",
      "xi_L tensor([18.7365,  0.1003,  1.6372,  1.0667], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 3/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.6833324432373047\n",
      "xi_L tensor([18.7651,  0.0953,  1.6914,  1.1044], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 4/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.6433706283569336\n",
      "xi_L tensor([18.7924,  0.0904,  1.7382,  1.1374], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 5/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.6107895374298096\n",
      "xi_L tensor([18.8185,  0.0858,  1.7785,  1.1662], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 6/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.5840694904327393\n",
      "xi_L tensor([18.8433,  0.0817,  1.8134,  1.1910], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 7/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.56209397315979\n",
      "xi_L tensor([18.8667,  0.0782,  1.8433,  1.2125], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 8/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.543893575668335\n",
      "xi_L tensor([18.8893,  0.0749,  1.8691,  1.2310], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 9/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.528756618499756\n",
      "xi_L tensor([18.9111,  0.0721,  1.8911,  1.2468], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 10/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.51611328125\n",
      "xi_L tensor([18.9321,  0.0695,  1.9099,  1.2604], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 11/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.505486488342285\n",
      "xi_L tensor([18.9527,  0.0673,  1.9260,  1.2721], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 12/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4964752197265625\n",
      "xi_L tensor([18.9730,  0.0653,  1.9396,  1.2821], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 13/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.488832473754883\n",
      "xi_L tensor([18.9927,  0.0635,  1.9512,  1.2906], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 14/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.482343912124634\n",
      "xi_L tensor([19.0120,  0.0618,  1.9608,  1.2977], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 15/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4768598079681396\n",
      "xi_L tensor([19.0303,  0.0604,  1.9688,  1.3037], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 16/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4721732139587402\n",
      "xi_L tensor([19.0479,  0.0591,  1.9754,  1.3087], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 17/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.468167304992676\n",
      "xi_L tensor([19.0647,  0.0579,  1.9807,  1.3127], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 18/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.464763641357422\n",
      "xi_L tensor([19.0813,  0.0570,  1.9850,  1.3160], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 19/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.461815118789673\n",
      "xi_L tensor([19.0975,  0.0560,  1.9883,  1.3187], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 20/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4593029022216797\n",
      "xi_L tensor([19.1123,  0.0552,  1.9908,  1.3207], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 21/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4571385383605957\n",
      "xi_L tensor([19.1271,  0.0545,  1.9928,  1.3223], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 22/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4552595615386963\n",
      "xi_L tensor([19.1408,  0.0539,  1.9941,  1.3235], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 23/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4536514282226562\n",
      "xi_L tensor([19.1535,  0.0534,  1.9948,  1.3243], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 24/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4522628784179688\n",
      "xi_L tensor([19.1660,  0.0529,  1.9952,  1.3248], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 25/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.451054096221924\n",
      "xi_L tensor([19.1778,  0.0525,  1.9952,  1.3250], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 26/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4499974250793457\n",
      "xi_L tensor([19.1894,  0.0521,  1.9949,  1.3250], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 27/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4490718841552734\n",
      "xi_L tensor([19.2007,  0.0518,  1.9944,  1.3248], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 28/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4482529163360596\n",
      "xi_L tensor([19.2112,  0.0515,  1.9936,  1.3244], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 29/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.447540521621704\n",
      "xi_L tensor([19.2209,  0.0512,  1.9927,  1.3239], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 30/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4469082355499268\n",
      "xi_L tensor([19.2304,  0.0510,  1.9916,  1.3233], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 31/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.446338176727295\n",
      "xi_L tensor([19.2397,  0.0509,  1.9904,  1.3226], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 32/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4458277225494385\n",
      "xi_L tensor([19.2484,  0.0507,  1.9892,  1.3218], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 33/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4453604221343994\n",
      "xi_L tensor([19.2570,  0.0506,  1.9879,  1.3209], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 34/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4449281692504883\n",
      "xi_L tensor([19.2654,  0.0505,  1.9864,  1.3200], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 35/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.444550037384033\n",
      "xi_L tensor([19.2728,  0.0504,  1.9850,  1.3190], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 36/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.444199800491333\n",
      "xi_L tensor([19.2791,  0.0504,  1.9835,  1.3180], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 37/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.443854331970215\n",
      "xi_L tensor([19.2854,  0.0504,  1.9820,  1.3171], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 38/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4435083866119385\n",
      "xi_L tensor([19.2915,  0.0504,  1.9806,  1.3161], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 39/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4431679248809814\n",
      "xi_L tensor([19.2973,  0.0505,  1.9792,  1.3151], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 40/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4428417682647705\n",
      "xi_L tensor([19.3028,  0.0505,  1.9778,  1.3141], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 41/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4425208568573\n",
      "xi_L tensor([19.3077,  0.0506,  1.9764,  1.3132], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 42/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4421916007995605\n",
      "xi_L tensor([19.3120,  0.0507,  1.9751,  1.3122], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 43/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4418606758117676\n",
      "xi_L tensor([19.3163,  0.0508,  1.9738,  1.3113], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 44/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.441537618637085\n",
      "xi_L tensor([19.3206,  0.0509,  1.9725,  1.3103], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 45/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4412143230438232\n",
      "xi_L tensor([19.3247,  0.0510,  1.9712,  1.3094], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 46/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4408962726593018\n",
      "xi_L tensor([19.3286,  0.0511,  1.9699,  1.3084], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 47/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.440577983856201\n",
      "xi_L tensor([19.3324,  0.0512,  1.9686,  1.3075], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 48/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4402568340301514\n",
      "xi_L tensor([19.3363,  0.0514,  1.9674,  1.3066], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 49/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4399263858795166\n",
      "xi_L tensor([19.3401,  0.0515,  1.9662,  1.3058], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 50/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4395928382873535\n",
      "xi_L tensor([19.3438,  0.0516,  1.9651,  1.3049], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 51/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4392566680908203\n",
      "xi_L tensor([19.3474,  0.0517,  1.9640,  1.3041], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 52/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4389266967773438\n",
      "xi_L tensor([19.3509,  0.0518,  1.9628,  1.3032], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 53/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4385998249053955\n",
      "xi_L tensor([19.3544,  0.0520,  1.9617,  1.3024], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 54/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4382741451263428\n",
      "xi_L tensor([19.3576,  0.0521,  1.9606,  1.3015], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 55/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4379451274871826\n",
      "xi_L tensor([19.3607,  0.0522,  1.9595,  1.3007], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 56/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4376089572906494\n",
      "xi_L tensor([19.3638,  0.0524,  1.9585,  1.2999], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 57/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.437269926071167\n",
      "xi_L tensor([19.3667,  0.0526,  1.9575,  1.2992], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 58/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.436927318572998\n",
      "xi_L tensor([19.3696,  0.0527,  1.9565,  1.2984], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 59/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4365901947021484\n",
      "xi_L tensor([19.3725,  0.0529,  1.9555,  1.2976], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 60/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4362542629241943\n",
      "xi_L tensor([19.3754,  0.0530,  1.9545,  1.2968], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 61/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4359169006347656\n",
      "xi_L tensor([19.3783,  0.0532,  1.9535,  1.2960], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 62/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.435577630996704\n",
      "xi_L tensor([19.3811,  0.0533,  1.9526,  1.2953], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 63/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4352355003356934\n",
      "xi_L tensor([19.3839,  0.0535,  1.9517,  1.2946], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 64/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4348971843719482\n",
      "xi_L tensor([19.3866,  0.0536,  1.9507,  1.2938], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 65/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4345593452453613\n",
      "xi_L tensor([19.3894,  0.0538,  1.9498,  1.2931], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 66/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.434215784072876\n",
      "xi_L tensor([19.3910,  0.0539,  1.9489,  1.2924], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 67/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.433863639831543\n",
      "xi_L tensor([19.3925,  0.0541,  1.9481,  1.2917], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 68/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4335057735443115\n",
      "xi_L tensor([19.3946,  0.0543,  1.9472,  1.2910], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 69/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4331464767456055\n",
      "xi_L tensor([19.3962,  0.0545,  1.9464,  1.2904], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 70/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4327876567840576\n",
      "xi_L tensor([19.3976,  0.0547,  1.9456,  1.2897], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 71/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4324238300323486\n",
      "xi_L tensor([19.3984,  0.0549,  1.9448,  1.2890], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 72/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.432044744491577\n",
      "xi_L tensor([19.3998,  0.0551,  1.9441,  1.2884], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 73/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4316678047180176\n",
      "xi_L tensor([19.4012,  0.0553,  1.9434,  1.2877], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 74/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4312903881073\n",
      "xi_L tensor([19.4025,  0.0555,  1.9427,  1.2871], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 75/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4309074878692627\n",
      "xi_L tensor([19.4039,  0.0557,  1.9420,  1.2866], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 76/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.430509328842163\n",
      "xi_L tensor([19.4052,  0.0559,  1.9414,  1.2860], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 77/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.43011474609375\n",
      "xi_L tensor([19.4065,  0.0561,  1.9407,  1.2854], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 78/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4297304153442383\n",
      "xi_L tensor([19.4078,  0.0562,  1.9401,  1.2849], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 79/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4293482303619385\n",
      "xi_L tensor([19.4092,  0.0564,  1.9394,  1.2843], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 80/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.428966760635376\n",
      "xi_L tensor([19.4099,  0.0566,  1.9388,  1.2838], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 81/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4285788536071777\n",
      "xi_L tensor([19.4112,  0.0568,  1.9382,  1.2832], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 82/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.428192615509033\n",
      "xi_L tensor([19.4121,  0.0570,  1.9376,  1.2827], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 83/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4278042316436768\n",
      "xi_L tensor([19.4125,  0.0572,  1.9370,  1.2821], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 84/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4274184703826904\n",
      "xi_L tensor([19.4138,  0.0574,  1.9364,  1.2816], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 85/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4270336627960205\n",
      "xi_L tensor([19.4142,  0.0576,  1.9359,  1.2811], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 86/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.426637649536133\n",
      "xi_L tensor([19.4146,  0.0579,  1.9353,  1.2805], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 87/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.426234245300293\n",
      "xi_L tensor([19.4155,  0.0581,  1.9349,  1.2801], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 88/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.425823450088501\n",
      "xi_L tensor([19.4159,  0.0583,  1.9344,  1.2796], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 89/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4254209995269775\n",
      "xi_L tensor([19.4163,  0.0585,  1.9339,  1.2791], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 90/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.425025701522827\n",
      "xi_L tensor([19.4170,  0.0587,  1.9333,  1.2786], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 91/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4246349334716797\n",
      "xi_L tensor([19.4174,  0.0589,  1.9328,  1.2781], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 92/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4242374897003174\n",
      "xi_L tensor([19.4181,  0.0591,  1.9324,  1.2776], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 93/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.423846960067749\n",
      "xi_L tensor([19.4184,  0.0594,  1.9319,  1.2772], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 94/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4234459400177\n",
      "xi_L tensor([19.4188,  0.0596,  1.9315,  1.2767], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 95/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.423038959503174\n",
      "xi_L tensor([19.4195,  0.0598,  1.9310,  1.2763], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 96/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4226431846618652\n",
      "xi_L tensor([19.4199,  0.0600,  1.9306,  1.2758], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 97/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.422253131866455\n",
      "xi_L tensor([19.4202,  0.0602,  1.9301,  1.2754], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 98/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4218499660491943\n",
      "xi_L tensor([19.4204,  0.0604,  1.9298,  1.2750], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 99/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4214437007904053\n",
      "xi_L tensor([19.4206,  0.0606,  1.9294,  1.2746], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 2\n",
      "Epoch 100/100\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.421039342880249\n",
      "xi_L tensor([19.4209,  0.0608,  1.9290,  1.2741], device='cuda:5')\n",
      "Result stage 2: 19.421*cos(x0)+1.929*x1_t**2+1.274*x0_t*cos(x0)*x1_t\n",
      "simplified :  1.274*x0_t*x1_t*cos(x0) + 1.929*x1_t**2 + 19.421*cos(x0)\n",
      "Dissipation :  0.96381414*x0_t**2 + 0.7811944*x1_t**2\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 0/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.443516492843628\n",
      "xi_L tensor([19.4279,  1.9598,  1.3346], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 1/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4240951538085938\n",
      "xi_L tensor([19.4322,  2.0015,  1.3637], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 2/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4132168292999268\n",
      "xi_L tensor([19.4315,  2.0387,  1.3875], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 3/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.4049103260040283\n",
      "xi_L tensor([19.4270,  2.0715,  1.4083], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 4/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3984217643737793\n",
      "xi_L tensor([19.4179,  2.0999,  1.4264], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 5/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3932902812957764\n",
      "xi_L tensor([19.4084,  2.1250,  1.4424], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 6/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.389155864715576\n",
      "xi_L tensor([19.3964,  2.1471,  1.4565], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 7/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3857274055480957\n",
      "xi_L tensor([19.3830,  2.1670,  1.4692], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 8/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3828351497650146\n",
      "xi_L tensor([19.3688,  2.1845,  1.4804], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 9/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3803789615631104\n",
      "xi_L tensor([19.3543,  2.2002,  1.4906], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 10/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.378246545791626\n",
      "xi_L tensor([19.3399,  2.2144,  1.4997], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 11/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.376375675201416\n",
      "xi_L tensor([19.3251,  2.2271,  1.5078], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 12/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.374725103378296\n",
      "xi_L tensor([19.3108,  2.2384,  1.5151], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 13/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3732519149780273\n",
      "xi_L tensor([19.2961,  2.2486,  1.5217], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 14/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.371889352798462\n",
      "xi_L tensor([19.2805,  2.2580,  1.5278], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 15/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.37060284614563\n",
      "xi_L tensor([19.2650,  2.2666,  1.5334], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 16/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.369400978088379\n",
      "xi_L tensor([19.2495,  2.2745,  1.5385], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 17/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.368269205093384\n",
      "xi_L tensor([19.2339,  2.2819,  1.5433], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 18/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3671932220458984\n",
      "xi_L tensor([19.2184,  2.2887,  1.5478], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 19/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.366177797317505\n",
      "xi_L tensor([19.2037,  2.2951,  1.5519], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 20/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3652267456054688\n",
      "xi_L tensor([19.1890,  2.3010,  1.5558], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 21/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3643128871917725\n",
      "xi_L tensor([19.1743,  2.3065,  1.5594], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 22/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3634376525878906\n",
      "xi_L tensor([19.1602,  2.3117,  1.5628], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 23/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.362609624862671\n",
      "xi_L tensor([19.1463,  2.3165,  1.5660], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 24/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.361825466156006\n",
      "xi_L tensor([19.1335,  2.3210,  1.5689], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 25/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3610873222351074\n",
      "xi_L tensor([19.1207,  2.3252,  1.5717], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 26/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.360370397567749\n",
      "xi_L tensor([19.1080,  2.3293,  1.5744], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 27/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.359673261642456\n",
      "xi_L tensor([19.0958,  2.3332,  1.5770], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 28/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3590097427368164\n",
      "xi_L tensor([19.0836,  2.3369,  1.5794], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 29/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.358375310897827\n",
      "xi_L tensor([19.0731,  2.3404,  1.5817], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 30/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3577938079833984\n",
      "xi_L tensor([19.0630,  2.3438,  1.5840], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 31/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3572347164154053\n",
      "xi_L tensor([19.0531,  2.3470,  1.5861], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 32/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.356692314147949\n",
      "xi_L tensor([19.0436,  2.3501,  1.5881], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 33/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3561742305755615\n",
      "xi_L tensor([19.0346,  2.3530,  1.5900], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 34/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3556787967681885\n",
      "xi_L tensor([19.0260,  2.3559,  1.5919], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 35/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3552026748657227\n",
      "xi_L tensor([19.0174,  2.3587,  1.5937], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 36/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3547370433807373\n",
      "xi_L tensor([19.0091,  2.3613,  1.5955], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 37/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3542871475219727\n",
      "xi_L tensor([19.0014,  2.3639,  1.5972], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 38/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3538646697998047\n",
      "xi_L tensor([18.9942,  2.3663,  1.5987], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 39/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.353459596633911\n",
      "xi_L tensor([18.9873,  2.3686,  1.6003], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 40/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3530681133270264\n",
      "xi_L tensor([18.9804,  2.3708,  1.6017], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 41/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3526840209960938\n",
      "xi_L tensor([18.9735,  2.3729,  1.6031], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 42/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3523077964782715\n",
      "xi_L tensor([18.9666,  2.3749,  1.6044], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 43/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.351937770843506\n",
      "xi_L tensor([18.9597,  2.3769,  1.6057], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 44/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.351574659347534\n",
      "xi_L tensor([18.9528,  2.3788,  1.6070], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 45/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3512215614318848\n",
      "xi_L tensor([18.9464,  2.3806,  1.6082], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 46/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.350881814956665\n",
      "xi_L tensor([18.9402,  2.3824,  1.6094], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 47/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.350551128387451\n",
      "xi_L tensor([18.9341,  2.3841,  1.6106], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 48/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3502285480499268\n",
      "xi_L tensor([18.9280,  2.3859,  1.6117], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 49/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.349911689758301\n",
      "xi_L tensor([18.9219,  2.3876,  1.6129], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 50/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.349600315093994\n",
      "xi_L tensor([18.9158,  2.3892,  1.6139], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 51/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.349294900894165\n",
      "xi_L tensor([18.9100,  2.3908,  1.6150], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 52/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3489997386932373\n",
      "xi_L tensor([18.9042,  2.3923,  1.6160], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 53/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3487091064453125\n",
      "xi_L tensor([18.8984,  2.3938,  1.6170], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 54/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3484249114990234\n",
      "xi_L tensor([18.8927,  2.3953,  1.6180], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 55/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.34814715385437\n",
      "xi_L tensor([18.8871,  2.3967,  1.6189], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 56/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.347874164581299\n",
      "xi_L tensor([18.8817,  2.3980,  1.6198], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 57/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.347609519958496\n",
      "xi_L tensor([18.8764,  2.3994,  1.6207], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 58/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3473522663116455\n",
      "xi_L tensor([18.8712,  2.4007,  1.6216], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 59/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3470990657806396\n",
      "xi_L tensor([18.8660,  2.4021,  1.6225], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 60/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.346850633621216\n",
      "xi_L tensor([18.8610,  2.4034,  1.6233], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 61/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3466103076934814\n",
      "xi_L tensor([18.8561,  2.4047,  1.6242], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 62/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3463737964630127\n",
      "xi_L tensor([18.8511,  2.4059,  1.6250], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 63/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3461415767669678\n",
      "xi_L tensor([18.8465,  2.4071,  1.6258], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 64/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3459181785583496\n",
      "xi_L tensor([18.8419,  2.4083,  1.6266], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 65/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3457019329071045\n",
      "xi_L tensor([18.8379,  2.4094,  1.6273], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 66/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.345494031906128\n",
      "xi_L tensor([18.8339,  2.4105,  1.6280], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 67/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3452911376953125\n",
      "xi_L tensor([18.8299,  2.4115,  1.6287], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 68/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3450918197631836\n",
      "xi_L tensor([18.8260,  2.4126,  1.6294], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 69/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.344895601272583\n",
      "xi_L tensor([18.8221,  2.4136,  1.6301], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 70/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.344703435897827\n",
      "xi_L tensor([18.8186,  2.4147,  1.6308], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 71/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3445193767547607\n",
      "xi_L tensor([18.8151,  2.4156,  1.6314], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 72/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3443377017974854\n",
      "xi_L tensor([18.8116,  2.4166,  1.6321], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 73/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3441591262817383\n",
      "xi_L tensor([18.8081,  2.4175,  1.6327], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 74/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3439865112304688\n",
      "xi_L tensor([18.8052,  2.4184,  1.6332], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 75/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3438193798065186\n",
      "xi_L tensor([18.8017,  2.4192,  1.6338], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 76/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3436508178710938\n",
      "xi_L tensor([18.7988,  2.4201,  1.6344], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 77/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.343491792678833\n",
      "xi_L tensor([18.7959,  2.4209,  1.6349], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 78/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.343334674835205\n",
      "xi_L tensor([18.7931,  2.4216,  1.6354], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 79/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.343179702758789\n",
      "xi_L tensor([18.7902,  2.4224,  1.6359], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 80/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.343027353286743\n",
      "xi_L tensor([18.7873,  2.4231,  1.6364], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 81/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.342876434326172\n",
      "xi_L tensor([18.7847,  2.4237,  1.6368], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 82/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3427295684814453\n",
      "xi_L tensor([18.7820,  2.4244,  1.6372], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 83/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3425850868225098\n",
      "xi_L tensor([18.7794,  2.4250,  1.6377], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 84/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3424429893493652\n",
      "xi_L tensor([18.7767,  2.4257,  1.6381], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 85/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.342302083969116\n",
      "xi_L tensor([18.7740,  2.4263,  1.6385], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 86/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3421623706817627\n",
      "xi_L tensor([18.7714,  2.4269,  1.6389], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 87/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3420252799987793\n",
      "xi_L tensor([18.7687,  2.4275,  1.6393], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 88/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3418898582458496\n",
      "xi_L tensor([18.7660,  2.4282,  1.6398], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 89/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3417556285858154\n",
      "xi_L tensor([18.7634,  2.4288,  1.6402], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 90/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.341623067855835\n",
      "xi_L tensor([18.7607,  2.4294,  1.6405], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 91/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3414955139160156\n",
      "xi_L tensor([18.7587,  2.4299,  1.6409], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 92/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3413753509521484\n",
      "xi_L tensor([18.7567,  2.4305,  1.6413], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 93/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3412559032440186\n",
      "xi_L tensor([18.7548,  2.4311,  1.6417], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 94/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3411383628845215\n",
      "xi_L tensor([18.7528,  2.4316,  1.6420], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 95/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.341022491455078\n",
      "xi_L tensor([18.7508,  2.4322,  1.6424], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 96/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.340907335281372\n",
      "xi_L tensor([18.7488,  2.4328,  1.6428], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 97/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.340794086456299\n",
      "xi_L tensor([18.7468,  2.4333,  1.6431], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 98/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.340681314468384\n",
      "xi_L tensor([18.7448,  2.4338,  1.6434], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 99/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3405709266662598\n",
      "xi_L tensor([18.7428,  2.4342,  1.6438], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 100/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.340460777282715\n",
      "xi_L tensor([18.7411,  2.4347,  1.6441], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 101/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3403549194335938\n",
      "xi_L tensor([18.7393,  2.4351,  1.6443], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 102/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.340250015258789\n",
      "xi_L tensor([18.7376,  2.4355,  1.6446], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 103/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.34014630317688\n",
      "xi_L tensor([18.7359,  2.4359,  1.6449], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 104/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3400399684906006\n",
      "xi_L tensor([18.7334,  2.4363,  1.6451], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 105/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.339932918548584\n",
      "xi_L tensor([18.7317,  2.4367,  1.6454], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 106/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3398287296295166\n",
      "xi_L tensor([18.7293,  2.4371,  1.6457], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 107/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.339721441268921\n",
      "xi_L tensor([18.7268,  2.4375,  1.6459], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 108/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3396155834198\n",
      "xi_L tensor([18.7249,  2.4379,  1.6462], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 109/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3395156860351562\n",
      "xi_L tensor([18.7230,  2.4384,  1.6465], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 110/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.33941650390625\n",
      "xi_L tensor([18.7210,  2.4388,  1.6468], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 111/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3393185138702393\n",
      "xi_L tensor([18.7191,  2.4391,  1.6470], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 112/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.339221239089966\n",
      "xi_L tensor([18.7171,  2.4394,  1.6472], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 113/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3391244411468506\n",
      "xi_L tensor([18.7152,  2.4398,  1.6475], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 114/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3390285968780518\n",
      "xi_L tensor([18.7133,  2.4401,  1.6477], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 115/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3389334678649902\n",
      "xi_L tensor([18.7113,  2.4405,  1.6479], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 116/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3388419151306152\n",
      "xi_L tensor([18.7101,  2.4408,  1.6481], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 117/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3387563228607178\n",
      "xi_L tensor([18.7088,  2.4411,  1.6484], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 118/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3386709690093994\n",
      "xi_L tensor([18.7075,  2.4415,  1.6486], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 119/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3385868072509766\n",
      "xi_L tensor([18.7062,  2.4419,  1.6488], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 120/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.33850359916687\n",
      "xi_L tensor([18.7049,  2.4422,  1.6490], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 121/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3384201526641846\n",
      "xi_L tensor([18.7037,  2.4425,  1.6493], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 122/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3383398056030273\n",
      "xi_L tensor([18.7029,  2.4428,  1.6494], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 123/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.338263511657715\n",
      "xi_L tensor([18.7022,  2.4430,  1.6496], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 124/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.338189125061035\n",
      "xi_L tensor([18.7014,  2.4432,  1.6497], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 125/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.338114023208618\n",
      "xi_L tensor([18.7006,  2.4434,  1.6499], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 126/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.338040590286255\n",
      "xi_L tensor([18.6999,  2.4436,  1.6500], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 127/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3379664421081543\n",
      "xi_L tensor([18.6991,  2.4437,  1.6501], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 128/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.33789324760437\n",
      "xi_L tensor([18.6984,  2.4438,  1.6502], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 129/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.337820291519165\n",
      "xi_L tensor([18.6976,  2.4440,  1.6503], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 130/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.337747573852539\n",
      "xi_L tensor([18.6968,  2.4442,  1.6504], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 131/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.337677240371704\n",
      "xi_L tensor([18.6961,  2.4443,  1.6505], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 132/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3376057147979736\n",
      "xi_L tensor([18.6953,  2.4445,  1.6506], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 133/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.337536334991455\n",
      "xi_L tensor([18.6945,  2.4446,  1.6507], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 134/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.337465524673462\n",
      "xi_L tensor([18.6938,  2.4447,  1.6507], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 135/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3373963832855225\n",
      "xi_L tensor([18.6930,  2.4448,  1.6508], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 136/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.337327003479004\n",
      "xi_L tensor([18.6922,  2.4448,  1.6508], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 137/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3372585773468018\n",
      "xi_L tensor([18.6915,  2.4449,  1.6509], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 138/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3371901512145996\n",
      "xi_L tensor([18.6907,  2.4450,  1.6509], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 139/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3371224403381348\n",
      "xi_L tensor([18.6900,  2.4452,  1.6510], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 140/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3370559215545654\n",
      "xi_L tensor([18.6892,  2.4453,  1.6511], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 141/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.336989164352417\n",
      "xi_L tensor([18.6884,  2.4454,  1.6512], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 142/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.336923122406006\n",
      "xi_L tensor([18.6877,  2.4455,  1.6513], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 143/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3368570804595947\n",
      "xi_L tensor([18.6869,  2.4456,  1.6513], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 144/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.336792230606079\n",
      "xi_L tensor([18.6861,  2.4457,  1.6514], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 145/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3367269039154053\n",
      "xi_L tensor([18.6854,  2.4459,  1.6515], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 146/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.336662769317627\n",
      "xi_L tensor([18.6846,  2.4461,  1.6516], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 147/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3365986347198486\n",
      "xi_L tensor([18.6839,  2.4463,  1.6518], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 148/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3365352153778076\n",
      "xi_L tensor([18.6831,  2.4464,  1.6519], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 149/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3364720344543457\n",
      "xi_L tensor([18.6824,  2.4466,  1.6520], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 150/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3364109992980957\n",
      "xi_L tensor([18.6818,  2.4468,  1.6521], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 151/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.336350679397583\n",
      "xi_L tensor([18.6812,  2.4470,  1.6522], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 152/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.336289882659912\n",
      "xi_L tensor([18.6805,  2.4471,  1.6523], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 153/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.336230516433716\n",
      "xi_L tensor([18.6799,  2.4472,  1.6524], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 154/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.336169719696045\n",
      "xi_L tensor([18.6792,  2.4473,  1.6524], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 155/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3361103534698486\n",
      "xi_L tensor([18.6786,  2.4473,  1.6525], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 156/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3360507488250732\n",
      "xi_L tensor([18.6779,  2.4474,  1.6525], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 157/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335991859436035\n",
      "xi_L tensor([18.6773,  2.4475,  1.6526], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 158/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335933208465576\n",
      "xi_L tensor([18.6766,  2.4476,  1.6526], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 159/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335874557495117\n",
      "xi_L tensor([18.6760,  2.4477,  1.6527], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 160/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3358170986175537\n",
      "xi_L tensor([18.6754,  2.4477,  1.6527], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 161/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335758686065674\n",
      "xi_L tensor([18.6747,  2.4478,  1.6528], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 162/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3357017040252686\n",
      "xi_L tensor([18.6741,  2.4479,  1.6528], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 163/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335644483566284\n",
      "xi_L tensor([18.6734,  2.4480,  1.6529], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 164/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335588216781616\n",
      "xi_L tensor([18.6728,  2.4480,  1.6529], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 165/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.33553147315979\n",
      "xi_L tensor([18.6721,  2.4481,  1.6530], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 166/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335475206375122\n",
      "xi_L tensor([18.6715,  2.4482,  1.6530], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 167/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3354194164276123\n",
      "xi_L tensor([18.6708,  2.4483,  1.6531], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 168/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3353641033172607\n",
      "xi_L tensor([18.6702,  2.4484,  1.6531], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 169/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3353092670440674\n",
      "xi_L tensor([18.6696,  2.4485,  1.6532], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 170/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335254430770874\n",
      "xi_L tensor([18.6689,  2.4486,  1.6533], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 171/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3352010250091553\n",
      "xi_L tensor([18.6683,  2.4488,  1.6534], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 172/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335146903991699\n",
      "xi_L tensor([18.6676,  2.4489,  1.6535], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 173/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3350963592529297\n",
      "xi_L tensor([18.6677,  2.4490,  1.6536], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 174/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.335050106048584\n",
      "xi_L tensor([18.6677,  2.4490,  1.6536], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 175/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3350000381469727\n",
      "xi_L tensor([18.6671,  2.4491,  1.6536], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 176/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.334949254989624\n",
      "xi_L tensor([18.6671,  2.4491,  1.6536], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 177/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3349032402038574\n",
      "xi_L tensor([18.6672,  2.4492,  1.6536], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 178/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.334857702255249\n",
      "xi_L tensor([18.6672,  2.4492,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 179/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3348119258880615\n",
      "xi_L tensor([18.6673,  2.4492,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 180/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3347666263580322\n",
      "xi_L tensor([18.6673,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 181/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.334718942642212\n",
      "xi_L tensor([18.6667,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 182/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.334669589996338\n",
      "xi_L tensor([18.6668,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 183/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.334624767303467\n",
      "xi_L tensor([18.6668,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 184/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.334580421447754\n",
      "xi_L tensor([18.6669,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 185/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.33453631401062\n",
      "xi_L tensor([18.6669,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 186/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3344926834106445\n",
      "xi_L tensor([18.6670,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 187/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3344483375549316\n",
      "xi_L tensor([18.6670,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 188/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3344037532806396\n",
      "xi_L tensor([18.6664,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 189/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.334355592727661\n",
      "xi_L tensor([18.6664,  2.4493,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 190/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.334310293197632\n",
      "xi_L tensor([18.6658,  2.4492,  1.6537], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 191/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3342628479003906\n",
      "xi_L tensor([18.6659,  2.4492,  1.6536], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 192/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3342177867889404\n",
      "xi_L tensor([18.6652,  2.4492,  1.6536], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 193/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3341708183288574\n",
      "xi_L tensor([18.6653,  2.4492,  1.6536], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 194/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3341283798217773\n",
      "xi_L tensor([18.6653,  2.4491,  1.6536], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 195/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3340842723846436\n",
      "xi_L tensor([18.6647,  2.4491,  1.6535], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 196/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3340377807617188\n",
      "xi_L tensor([18.6648,  2.4490,  1.6535], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 197/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3339943885803223\n",
      "xi_L tensor([18.6641,  2.4490,  1.6535], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 198/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.3339483737945557\n",
      "xi_L tensor([18.6642,  2.4489,  1.6534], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 199/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.33390736579895\n",
      "xi_L tensor([18.6642,  2.4489,  1.6534], device='cuda:5')\n",
      "\n",
      "\n",
      "Stage 3\n",
      "Epoch 200/200\n",
      "Learning rate :  1.1000000000000001e-05\n",
      "Average loss :  3.333864212036133\n",
      "xi_L tensor([18.6636,  2.4488,  1.6534], device='cuda:5')\n",
      "thresholding using the simplified expression\n",
      "Result stage 3: 19.6*cos(x0)+2.572*x1_t**2+1.736*x0_t*x1_t*cos(x0)\n",
      "Dissipation :  0.94552416*x0_t**2 + 0.69889164*x1_t**2\n"
     ]
    }
   ],
   "source": [
    "for stage in range(20):\n",
    "\n",
    "    #Redefine computation after thresholding\n",
    "    expr.append(known_expr[0])\n",
    "    Zeta, Eta, Delta, Dissip = LagrangianLibraryTensor(X,Xdot,expr,d_expr,states,states_dot, scaling=False)\n",
    "    expr = np.array(expr)\n",
    "    i0 = np.where(expr == 'x0_t**2')[0]\n",
    "    i1 = np.where(expr == 'cos(x0)')[0]\n",
    "    idx = np.arange(0,len(expr))\n",
    "    idx = np.delete(idx,i0)\n",
    "    known_expr = expr[i0].tolist()  \n",
    "    expr = np.delete(expr,i0).tolist()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    nonpenaltyidx = [i1]\n",
    "\n",
    "    Zeta_ = Zeta[:,:,i0,:].clone().detach()\n",
    "    Eta_ = Eta[:,:,i0,:].clone().detach()\n",
    "    Delta_ = Delta[:,i0,:].clone().detach()\n",
    "\n",
    "    Zeta = Zeta[:,:,idx,:]\n",
    "    Eta = Eta[:,:,idx,:]\n",
    "    Delta = Delta[:,idx,:]\n",
    "\n",
    "\n",
    "\n",
    "    # nonpenaltyidx = []\n",
    "\n",
    "    Zeta = Zeta.to(device)\n",
    "    Eta = Eta.to(device)\n",
    "    Delta = Delta.to(device)\n",
    "    Zeta_ = Zeta_.to(device)\n",
    "    Eta_ = Eta_.to(device)\n",
    "    Delta_ = Delta_.to(device)\n",
    "\n",
    "    Dissip = Dissip.to(device)\n",
    "\n",
    "\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # if(len(xi_L)+len(xi_d) <= 6):\n",
    "    if len(xi_L) <= 3:\n",
    "        lam = 0\n",
    "        threshold = 1e-3\n",
    "        converged = True\n",
    "        Epoch = 200\n",
    "    # elif(len(xi_L) <= 8):\n",
    "    #     lam = 0\n",
    "    else:\n",
    "        threshold = 0.1\n",
    "        lr += lr_step\n",
    "        lam = lam\n",
    "    temp = 1000\n",
    "    RHS = [Zeta, Eta, Delta]\n",
    "    LHS = [Zeta_, Eta_, Delta_]\n",
    "    while(i<=Epoch):\n",
    "        if display:\n",
    "            print(\"\\n\")\n",
    "            print(\"Stage \" + str(stage+2))\n",
    "            print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "            print(\"Learning rate : \", lr)\n",
    "        xi_L, prevxi_L,xi_d, lossitem= PGD_loop(Tau, c, xi_L,prevxi_L,xi_d, RHS, LHS, Dissip, Xdot, batch_size, lr=lr,lam=lam,momentum=True,device=device,D_CAL=True)\n",
    "        i+=1\n",
    "        if display:\n",
    "            print('xi_L', xi_L)\n",
    "        #attend to loss list, if the size of the loss list is less than 10, append the loss value, else pop the first element and append the new loss value\n",
    "        if len(last_ten_loss) < 10:\n",
    "            last_ten_loss.append(lossitem)\n",
    "        else:\n",
    "            last_ten_loss.pop(0)\n",
    "            last_ten_loss.append(lossitem)\n",
    "        #calculate the changes in the loss value, if the all changes are less than a threshold, break the loop\n",
    "        if len(last_ten_loss) == 10:\n",
    "            if all(abs(last_ten_loss[i] - last_ten_loss[i+1]) < tol for i in range(len(last_ten_loss)-1)):\n",
    "                if display:\n",
    "                    print(\"training is converged\")\n",
    "                    print(\"last ten loss values : \",    last_ten_loss)\n",
    "                converged = True\n",
    "## Thresholding\n",
    "    if stage < 1 or len(xi_L) > 18:\n",
    "        #regularize the biggest coefficient to 20\n",
    "        idx = torch.argmax(torch.abs(xi_L))\n",
    "        xi_Ltemp = xi_L / xi_L[idx] * 19.6\n",
    "        xi_d = xi_d / xi_L[idx] * 19.6\n",
    "        Tau = Tau / xi_L[idx] * 19.6\n",
    "        surv_index = ((torch.abs(xi_Ltemp) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "        expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "        xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "        xi_d = xi_d.clone().detach().requires_grad_(True)\n",
    "        prevxi_L = xi_L.clone().detach()\n",
    "\n",
    "        ## obtaining analytical model\n",
    "        xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
    "        L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-2)\n",
    "        D = HL.generateExpression(xi_d.detach().cpu().numpy(),d_expr)\n",
    "        # print(\"Result stage \" + str(stage+2) + \":\" , simplify(L))\n",
    "        if display:\n",
    "            print(\"Result stage \" + str(stage+2) + \":\" , L)\n",
    "            print(\"simplified : \", simplify(L))\n",
    "            print(\"Dissipation : \", simplify(D))\n",
    "        if converged:\n",
    "            break\n",
    "    else:\n",
    "        if display:\n",
    "            print(\"thresholding using the simplified expression\")\n",
    "    ## Thresholding\n",
    "        ## obtaining analytical model\n",
    "        #calculate the relative threshold\n",
    "        scaler = 19.6 / torch.abs(xi_L).max().item()\n",
    "        xi_L = xi_L * scaler\n",
    "        xi_d = xi_d * scaler\n",
    "        Tau = Tau * scaler\n",
    "        xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
    "        L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-1)\n",
    "        D = HL.generateExpression(xi_d.detach().cpu().numpy(),d_expr)\n",
    "        L_simplified = simplify(L)\n",
    "        x0, x1,x0_t,x1_t = symbols('x0 x1 x0_t x1_t')\n",
    "        coeff_dict = L_simplified.as_coefficients_dict()\n",
    "        scaler = coeff_dict['cos(x0)']/20\n",
    "        relative_threshold = threshold * scaler\n",
    "        #check the value of the coefficients, if the value is less than the relative threshold, remove the term\n",
    "        filter_dict = {}\n",
    "        for key in coeff_dict.keys():\n",
    "            if abs(coeff_dict[key]) > relative_threshold:\n",
    "                filter_dict[key] = coeff_dict[key]\n",
    "        xi_L_value = list(filter_dict.values())\n",
    "        xi_L = torch.tensor(xi_L_value,device=device,dtype=torch.float32).requires_grad_(True)\n",
    "        expr_temp = list(filter_dict.keys())\n",
    "        expr =[]\n",
    "        for x in expr_temp:\n",
    "            expr.append('{}'.format(x))\n",
    "        xi_d = xi_d.clone().detach().requires_grad_(True)\n",
    "        prevxi_L = xi_L.clone().detach()\n",
    "        xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
    "        #perform thresholding on the dissipation term without simplification\n",
    "        surv_index = ((torch.abs(xi_d) >= threshold_d)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "        d_expr = np.array(d_expr)[surv_index].tolist()\n",
    "        xi_d =xi_d[surv_index].clone().detach().requires_grad_(True)\n",
    "        D = HL.generateExpression(xi_d.detach().cpu().numpy(),d_expr)\n",
    "\n",
    "    \n",
    "        L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-1)\n",
    "        if display:\n",
    "            print(\"Result stage \" + str(stage+2) + \":\" , L)\n",
    "            print(\"Dissipation : \", simplify(D))\n",
    "        if not sanity_check(expr):\n",
    "            if display:\n",
    "                print(\"sanity check failed\")\n",
    "            break\n",
    "        if converged:\n",
    "            total_epoch = (stage+1) * Epoch + Epoch0\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding known terms\n",
    "expr = np.array(expr)\n",
    "expr = np.append(expr, known_expr)\n",
    "scaler = torch.max(Tau_org)/torch.max(Tau)\n",
    "xi_L = torch.cat((xi_L, c), dim=0)\n",
    "xi_Ltemp = xi_L * scaler\n",
    "xi_d = xi_d * scaler\n",
    "xi_Lcpu = np.around(xi_Ltemp.detach().cpu().numpy(),decimals=3)\n",
    "L = HL.generateExpression(xi_Lcpu,expr)\n",
    "L = str(simplify(L)) \n",
    "D = HL.generateExpression(xi_d.detach().cpu().numpy(),d_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\m\n",
      "Obtained Lagrangian :  0.944*x0_t**2 + 1.638*x0_t*x1_t*cos(x0) + 2.427*x1_t**2 + 18.493*cos(x0)\n",
      "Obtained Dissipation :  0.8921226*x0_t**2 + 0.6594195*x1_t**2\n",
      "The relative errors are: 0.275686034658512\n"
     ]
    }
   ],
   "source": [
    "if display:\n",
    "    print(\"\\m\")\n",
    "    print(\"Obtained Lagrangian : \", L)\n",
    "    print(\"Obtained Dissipation : \", simplify(D))\n",
    "#caluclate the relative error of the obtained coefficients\n",
    "#the real Lagrangian model is m1*l1**2*x0_t**2/2 + m2*(l1**2*x0_t**2/2 + l2**2*x1_t**2/2 + l1*l2*x0_t*x1_t*cos(x0)*cos(x1)+l1*l2*x0_t*x1_t*sin(x0)*sin(x1)) + (m1+m2)*g*l1*cos(x0) + m2*g*l2*cos(x1)\n",
    "\n",
    "l, M, m, g = param['L'], param['M'], param['m'] , param['g']\n",
    "\n",
    "# Define the symbols\n",
    "x0, x1, x0_t, x1_t = symbols('x0 x1 x0_t x1_t')\n",
    "\n",
    "# Define the real Lagrangian model\n",
    "L_real = 0.5*(M+m)*x1_t**2+m*l*x0_t*x1_t*cos(x0)+0.5*m*l**2*x0_t**2+m*g*l*cos(x0)\n",
    "\n",
    "\n",
    "\n",
    "# Get the real coefficients\n",
    "real_coeff_dict = L_real.as_coefficients_dict()\n",
    "real_coeff_dict = {str(key): val for key, val in real_coeff_dict.items()}\n",
    "\n",
    "# Create a dictionary of estimated coefficients\n",
    "estimated_coeff_dict = filter_dict\n",
    "estimated_coeff_dict['x0_t**2'] = float(1.0)\n",
    "#change the keys of the estimated_coeff_dict to string\n",
    "estimated_coeff_dict = {str(key): val for key, val in estimated_coeff_dict.items()}\n",
    "\n",
    "#scale the x0_t**2 and use that scaler to scale the other coefficients\n",
    "scale = real_coeff_dict['x0_t**2']/estimated_coeff_dict['x0_t**2']\n",
    "\n",
    "for key in estimated_coeff_dict.keys():\n",
    "    estimated_coeff_dict[key] = estimated_coeff_dict[key]*scale\n",
    "\n",
    "# Calculate the relative error\n",
    "# Initialize the sum of relative errors\n",
    "sum_relative_errors = 0\n",
    "\n",
    "# Calculate the relative error for each coefficient\n",
    "for cand in estimated_coeff_dict.keys():\n",
    "    #check if the term is in the real coefficients\n",
    "    if cand in real_coeff_dict.keys():\n",
    "        real_coeff = real_coeff_dict[cand]\n",
    "        estimated_coeff = estimated_coeff_dict[cand]\n",
    "        relative_error = abs(real_coeff - estimated_coeff) / abs(real_coeff)\n",
    "        sum_relative_errors += relative_error\n",
    "    else:\n",
    "        if display:\n",
    "            print(f\"The term {cand} is not in the real coefficients\")\n",
    "        sum_relative_errors += 1\n",
    "\n",
    "# Print the relative errors\n",
    "if display:\n",
    "    print(\"The relative errors are:\", sum_relative_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
